# PHASE 5: CORE BACKEND IMPLEMENTATION - DETAILED PLAN

**VersiÃ³n:** 1.0
**Fecha CreaciÃ³n:** 2025-10-22
**Estado:** ðŸ”µ LISTO PARA INICIAR
**DuraciÃ³n Estimada:** 2-3 semanas
**Fecha Inicio:** 2025-10-23
**Fecha FinalizaciÃ³n Estimada:** 2025-11-09

---

## ðŸ“‘ TABLA DE CONTENIDOS

1. [Resumen Ejecutivo](#resumen-ejecutivo)
2. [Arquitectura de ImplementaciÃ³n](#arquitectura-de-implementaciÃ³n)
3. [Desglose de Tareas por Especialista](#desglose-de-tareas-por-especialista)
4. [Algoritmo de LLM Routing Hybrid](#algoritmo-de-llm-routing-hybrid)
5. [Estructura de CÃ³digo](#estructura-de-cÃ³digo)
6. [Dependencias y Setup](#dependencias-y-setup)
7. [Testing Strategy](#testing-strategy)
8. [Criterios de FinalizaciÃ³n](#criterios-de-finalizaciÃ³n)
9. [Riesgos y MitigaciÃ³n](#riesgos-y-mitigaciÃ³n)
10. [Cronograma Detallado](#cronograma-detallado)

---

## ðŸŽ¯ RESUMEN EJECUTIVO

### Objetivo Fase 5

Implementar la **lÃ³gica central del backend** que permite que el MVP funcione como un sistema integrado de:
- **Chat API** con streaming SSE en tiempo real
- **RAG (Retrieval-Augmented Generation)** integrado con Pinecone
- **Multi-LLM Routing** con algoritmo hÃ­brido (calidad + costo + disponibilidad)
- **AutenticaciÃ³n y RBAC** con NextAuth.js
- **Rate limiting** basado en tiers
- **Tests unitarios e integraciÃ³n** > 80% cobertura

### Dependencias Previas (Completadas)

âœ… **Fase 1:** Requirements & Technical Stack
âœ… **Fase 2:** Architecture Design (7-layer)
âœ… **Fase 3:** Database Design (54 tablas + Pinecone schema)
âœ… **Fase 4:** API Specification (58 endpoints, OpenAPI 3.0, RBAC matrix)

### Entregables Esperados

| Entregable | Tipo | Responsable | Estimado |
|-----------|------|-------------|----------|
| Chat API (POST /api/v1/chat/send) | CÃ³digo | coder | 4 dÃ­as |
| Chat Sessions CRUD | CÃ³digo | coder | 2 dÃ­as |
| LLM Router (Hybrid algorithm) | CÃ³digo | ai-specialist | 5 dÃ­as |
| RAG Document Upload | CÃ³digo | ai-specialist | 3 dÃ­as |
| RAG Semantic Search | CÃ³digo | ai-specialist | 3 dÃ­as |
| NextAuth Setup + RBAC | CÃ³digo | coder + security-specialist | 3 dÃ­as |
| Rate Limiting Middleware | CÃ³digo | coder | 2 dÃ­as |
| Unit + Integration Tests | CÃ³digo | coder + tester | 5 dÃ­as |
| Documentation Updates | DocumentaciÃ³n | documenter | 2 dÃ­as |

---

## ðŸ—ï¸ ARQUITECTURA DE IMPLEMENTACIÃ“N

### 1. Chat API con SSE Streaming

**Endpoint:** `POST /api/v1/chat/send`

```typescript
// Request
{
  "sessionId": "session-uuid",
  "message": "Explica RAG",
  "selectedAgents": ["claude-3.5-sonnet-20241022"],
  "ragEnabled": true,
  "temperature": 0.7,
  "maxTokens": 4096
}

// Response (Server-Sent Events)
event: start
data: {"messageId": "msg-uuid", "timestamp": "2025-10-22T10:30:00Z"}

event: chunk
data: {"content": "RAG stands for", "delta": "RAG stands for"}

event: chunk
data: {"content": "RAG stands for Retrieval", "delta": " Retrieval"}

event: done
data: {"messageId": "msg-uuid", "tokensUsed": 250, "completedAt": "..."}
```

**Flujo de EjecuciÃ³n:**
```
1. Validar request + auth
2. Obtener chat session
3. Guardar user message en DB
4. Si RAG enabled:
   a. Buscar documentos relevantes en Pinecone
   b. Construir context string
5. Seleccionar LLM model via router
6. Generar system prompt + context
7. Llamar a modelo vÃ­a Vercel AI SDK
8. Streamear response como SSE
9. Guardar assistant message + metadata en DB
10. Retornar tokens used + cost
```

### 2. LLM Routing Hybrid Algorithm

**Componentes del Algoritmo:**

```typescript
interface ModelScore {
  model: string;
  qualityScore: number;    // 0-1: capabilities match
  costScore: number;       // 0-1: cost efficiency (inverse)
  availabilityScore: number; // 0-1: uptime + latency
  finalScore: number;      // weighted average
}

// Scoring Logic:
const finalScore = (
  0.4 * qualityScore +     // 40% calidad del modelo
  0.3 * costScore +        // 30% eficiencia de costo
  0.3 * availabilityScore  // 30% disponibilidad/latencia
)

// Models Available:
const models = {
  "claude-3.5-sonnet-20241022": {
    quality: 0.95,         // Mejor para cÃ³digo, anÃ¡lisis
    costPer1kTokens: 0.003,
    avgLatency: 800        // ms
  },
  "gpt-4o": {
    quality: 0.92,         // VersÃ¡til, bueno para todo
    costPer1kTokens: 0.015,
    avgLatency: 1200
  },
  "gemini-2.0-flash": {
    quality: 0.88,         // RÃ¡pido, econÃ³mico
    costPer1kTokens: 0.001,
    avgLatency: 500
  },
  "deepseek-chat": {
    quality: 0.85,         // Alternativa econÃ³mica
    costPer1kTokens: 0.0005,
    avgLatency: 1500
  }
};

// Fallback Strategy:
// Si modelo primario falla â†’ intenta siguiente mejor score
// MÃ¡ximo 3 reintentos
// Timeout: 30 segundos por intento
```

**Reglas de SelecciÃ³n:**

```
IF user.tier = "FREE":
  â†’ Preferir Gemini (costo bajo)
  â†’ Evitar GPT-4 (costo alto)

IF user.tier = "PRO":
  â†’ Balancear calidad + costo
  â†’ Preferir Claude para tareas tÃ©cnicas
  â†’ Preferir Gemini para tareas rÃ¡pidas

IF user.tier = "ENTERPRISE":
  â†’ Preferir Claude (mejor calidad)
  â†’ Usar GPT-4 como fallback
  â†’ Disponible: Cualquier modelo

IF prompt_contains("code"):
  â†’ Boost Claude quality score +0.1

IF prompt_contains("imagen"):
  â†’ Preferir Gemini (soporte multimodal)

IF latency > 2000ms:
  â†’ Penalizar modelo (-0.2 score)
  â†’ Favorecer modelos rÃ¡pidos
```

### 3. RAG Pipeline

**Componentes:**

```
Document Upload â†’ Chunking â†’ Embedding â†’ Pinecone Storage â†’ Semantic Search

1. Document Upload (POST /api/v1/rag/documents)
   - Soportado: PDF, TXT, MD, DOCX, JSON, CSV
   - LÃ­mites por tier (10MB FREE, 100MB PRO)
   - Almacenamiento: Vercel Blob Storage

2. Chunking Strategy
   - TamaÃ±o chunks: 800 caracteres
   - Overlap: 200 caracteres
   - Preservar estructura (tÃ­tulos, tablas)
   - Metadata: source, page, section

3. Embedding Generation
   - Modelo: text-embedding-3-small (OpenAI)
   - Dimensiones: 1536
   - CachÃ© local para documentos duplicados

4. Pinecone Storage
   - Index: "cjhirashi-agents-mvp"
   - Namespace: "{userId}-{documentId}"
   - Metadata: document_id, user_id, chunk_index, source

5. Semantic Search (POST /api/v1/rag/search)
   - Query embedding via OpenAI
   - Threshold: 0.7 similarity
   - Retornar top-5 resultados
   - Include metadata + source

6. Context Injection
   - Top-5 chunks concatenados
   - Formato: "From [source] page [page]: [content]"
   - LÃ­mite: 2000 tokens mÃ¡ximo
   - Append a system prompt
```

### 4. Authentication & RBAC

**ImplementaciÃ³n NextAuth.js:**

```typescript
// .env.local
NEXTAUTH_URL=https://cjhirashi-agents.vercel.app
NEXTAUTH_SECRET=<generated>

// Providers:
1. CredentialsProvider (Email + Password)
2. GoogleProvider (OAuth)
3. GitHubProvider (OAuth)

// Session Strategy: JWT
// Token lifetime: 15 minutes (access) + 30 days (refresh)
// Cookie: httpOnly, secure, sameSite=lax

// RBAC Implementation:
enum UserRole {
  USER = "USER",
  ADMIN = "ADMIN",
  SUPER_ADMIN = "SUPER_ADMIN"
}

enum UserTier {
  FREE = "FREE",
  PRO = "PRO",
  ENTERPRISE = "ENTERPRISE"
}

// Middleware Guards:
- requireAuth(): Check JWT valid
- requireRole(role): Check user role
- requireTier(tier): Check subscription tier
- checkRateLimit(endpoint): Check quota
```

### 5. Rate Limiting

**Token Bucket Algorithm:**

```
FREE tier:
  - 20 requests/minute
  - 100 requests/hour
  - 1,000 requests/day
  - 100 messages/month
  - 5 documents/month

PRO tier:
  - 100 requests/minute
  - 1,000 requests/hour
  - 10,000 requests/day
  - Unlimited messages
  - 50 documents/month

ENTERPRISE tier:
  - 1,000 requests/minute
  - 10,000 requests/hour
  - Unlimited/day
  - Unlimited messages
  - Unlimited documents

// Response Headers:
X-RateLimit-Limit: 100
X-RateLimit-Remaining: 95
X-RateLimit-Reset: 1634567890
```

---

## ðŸŽ¯ DESGLOSE DE TAREAS POR ESPECIALISTA

### ESPECIALISTA 1: CODER (Backend Lead)

**Responsabilidad Total:** Chat API, endpoints CRUD, middleware, tests

**Tareas Detalladas:**

#### T1.1: Setup Next.js Backend Infrastructure (2 dÃ­as)
```
Subtareas:
- [ ] Crear estructura de carpetas (app/api/v1/*, lib/*, types/*)
- [ ] Setup Prisma client + conexiÃ³n PostgreSQL
- [ ] Setup Vercel AI SDK client
- [ ] Variables de entorno (.env.local, .env.production)
- [ ] Middleware de error handling global
- [ ] Logger setup (Winston o Pino)
- [ ] Setup TypeScript paths
```

**Deliverables:**
- Estructura de cÃ³digo lista
- ConfiguraciÃ³n funcionando
- Ejemplo de endpoint funcionando

#### T1.2: Implementar Chat API con SSE (4 dÃ­as)
```
Subtareas:
- [ ] POST /api/v1/chat/send (SSE streaming)
  - ValidaciÃ³n de request
  - Obtener chat session
  - Guardar user message
  - Llamar LLM via ai-specialist's router
  - Stream response como Server-Sent Events
  - Guardar assistant message + metadata
  - Calcular tokens + costo

- [ ] GET /api/v1/chat/sessions (lista sesiones)
- [ ] POST /api/v1/chat/sessions (crear sesiÃ³n)
- [ ] GET /api/v1/chat/sessions/{id} (detalle sesiÃ³n)
- [ ] DELETE /api/v1/chat/sessions/{id} (borrar sesiÃ³n)
- [ ] GET /api/v1/chat/history/{id} (historial con paginaciÃ³n)
```

**Deliverables:**
- Endpoint POST /chat/send funcionando con SSE
- CRUD completo para sessions
- Historial paginado funcionando

#### T1.3: Implementar NextAuth.js Setup (2 dÃ­as)
```
Subtareas:
- [ ] Configurar NextAuth con Credentials provider
- [ ] Configurar Google OAuth
- [ ] Configurar GitHub OAuth
- [ ] JWT token generation + refresh logic
- [ ] Session persistence en DB
- [ ] Callbacks para session enrichment
- [ ] Pages customizadas (signin, error, welcome)
```

**Deliverables:**
- Login/signup funcionando
- Token generation + refresh
- Session management

#### T1.4: Rate Limiting Middleware (2 dÃ­as)
```
Subtareas:
- [ ] Token bucket algorithm implementation
- [ ] Tier-based quota configuration
- [ ] Middleware para validar lÃ­mites
- [ ] Response headers (X-RateLimit-*)
- [ ] Error 429 handling
- [ ] Redis client para distributed rate limiting
```

**Deliverables:**
- Middleware funcionando
- Rate limits enforcement
- DocumentaciÃ³n de limits

#### T1.5: Unit + Integration Tests (4 dÃ­as)
```
Subtareas:
- [ ] Jest setup + test infrastructure
- [ ] Unit tests para endpoints (mocks)
- [ ] Integration tests (DB + LLM)
- [ ] E2E tests para chat flow completo
- [ ] Performance tests (latency SLAs)
- [ ] Error scenario tests
- [ ] Coverage report (target: 80%+)
```

**Deliverables:**
- Test suite > 80% cobertura
- Tests pasando localmente
- CI/CD pipeline ready

**Total Coder:** ~15 dÃ­as de trabajo

---

### ESPECIALISTA 2: AI-SPECIALIST

**Responsabilidad Total:** LLM routing, RAG integration, model integration

**Tareas Detalladas:**

#### T2.1: Hybrid LLM Router Implementation (5 dÃ­as)
```
Subtareas:
- [ ] Implementar scoring logic (quality, cost, availability)
- [ ] Integrar con Vercel AI SDK
- [ ] Implementar fallback strategy (max 3 retries)
- [ ] Model configuration management
- [ ] Cost tracking por modelo
- [ ] Latency monitoring
- [ ] Logging para anÃ¡lisis de decisiones
```

**Deliverables:**
- Router funcionando con algoritmo hybrid
- Fallback strategy implementada
- Cost + latency tracking

#### T2.2: RAG Document Management (3 dÃ­as)
```
Subtareas:
- [ ] POST /api/v1/rag/documents (upload + indexing)
  - File validation
  - Vercel Blob Storage upload
  - Trigger chunking + embedding
  - Save metadata en DB

- [ ] DELETE /api/v1/rag/documents/{id}
  - Eliminar documento
  - Limpiar embeddings de Pinecone
  - Limpiar Blob storage

- [ ] GET /api/v1/rag/documents (lista documentos)
```

**Deliverables:**
- Upload endpoint funcionando
- Document metadata en DB
- Blob storage integration

#### T2.3: RAG Chunking + Embedding Pipeline (3 dÃ­as)
```
Subtareas:
- [ ] Implementar document chunking
  - Detectar formato (PDF, MD, TXT, etc)
  - Splits por 800 caracteres + 200 overlap
  - Preservar metadata (pÃ¡gina, secciÃ³n)

- [ ] Embedding generation
  - Call OpenAI text-embedding-3-small
  - Cache para duplicates
  - Batch processing
```

**Deliverables:**
- Chunking working for all file types
- Embedding generation working
- Pipeline automated

#### T2.4: RAG Semantic Search (3 dÃ­as)
```
Subtareas:
- [ ] Pinecone index setup
  - Index name: "cjhirashi-agents-mvp"
  - Namespace strategy: userId-documentId
  - Metadata filtering

- [ ] POST /api/v1/rag/search (semantic search)
  - Query embedding generation
  - Pinecone vector search
  - Threshold filtering (0.7)
  - Return top-5 + metadata

- [ ] Context injection en prompts
  - Format context string
  - LÃ­mite 2000 tokens
  - Include source attribution
```

**Deliverables:**
- Pinecone integration working
- Search endpoint functioning
- Context injection in chat flow

#### T2.5: Multi-Model Integration (4 dÃ­as)
```
Subtareas:
- [ ] Claude integration via Vercel AI SDK
- [ ] GPT-4o integration
- [ ] Gemini integration
- [ ] DeepSeek integration
- [ ] Prompt optimization por modelo
- [ ] Token counting para billing
```

**Deliverables:**
- All 4 models working
- Prompts optimized
- Token tracking

**Total AI-Specialist:** ~10 dÃ­as de trabajo

---

### ESPECIALISTA 3: ARCHITECT (Oversight)

**Responsabilidad:** Validar que implementaciÃ³n siga arquitectura, resolver blockers

**Tareas:**

#### T3.1: Architecture Review & Validation (5 dÃ­as)
```
- [ ] Daily sync (15 min) con coder + ai-specialist
- [ ] Code review para architectural decisions
- [ ] Validar que layers se respetan
- [ ] Resolver conflictos de diseÃ±o
- [ ] Performance optimization suggestions
- [ ] Security review
```

**Deliverables:**
- Architecture maintained
- Blockers resolved
- Performance optimized

---

## ðŸ§  ALGORITMO DE LLM ROUTING HYBRID

### Scoring Components

```typescript
interface RoutingDecision {
  selectedModel: string;
  scores: {
    quality: number;      // 0-1
    cost: number;         // 0-1
    availability: number; // 0-1
    final: number;        // 0-1
  };
  reasoning: string;
  fallbacks: string[];    // ranked list
}

// QUALITY SCORE (40% weight)
// Basado en: capabilities, training data, specialization
const getQualityScore = (model: string, context: RequestContext) => {
  let score = baselineScore[model]; // Claude: 0.95, GPT-4o: 0.92, etc.

  // Boost para code tasks
  if (context.prompt.includes("code") && model.includes("claude")) {
    score += 0.1;
  }

  // Boost para anÃ¡lisis
  if (context.prompt.includes("anÃ¡lisis") && model.includes("gpt")) {
    score += 0.05;
  }

  // Reduce si latencia estÃ¡ alta
  if (model.avgLatency > 2000) {
    score -= 0.1;
  }

  return Math.min(1, score);
};

// COST SCORE (30% weight)
// Basado en: precio por token, tokens esperados
const getCostScore = (model: string, estimatedTokens: number) => {
  const cost = models[model].costPer1kTokens * (estimatedTokens / 1000);
  const maxCost = 0.10; // $0.10 es considerado "caro"

  // Score inverso: costo bajo = score alto
  const score = 1 - Math.min(1, cost / maxCost);

  return score;
};

// AVAILABILITY SCORE (30% weight)
// Basado en: uptime, latencia actual, queue depth
const getAvailabilityScore = (model: string, systemMetrics: any) => {
  let score = 1.0;

  // Restar por downtime
  score -= (1 - systemMetrics[model].uptime) * 0.5;

  // Restar por latencia alta
  if (systemMetrics[model].currentLatency > 3000) {
    score -= 0.3;
  } else if (systemMetrics[model].currentLatency > 1500) {
    score -= 0.1;
  }

  // Restar por queue depth
  if (systemMetrics[model].queueDepth > 100) {
    score -= 0.2;
  }

  return Math.max(0, Math.min(1, score));
};

// TIER-BASED CONSTRAINTS
const getTierConstraints = (tier: UserTier): string[] => {
  switch (tier) {
    case "FREE":
      return ["gemini-2.0-flash", "deepseek-chat"]; // EconÃ³micos
    case "PRO":
      return ["claude-3.5-sonnet", "gpt-4o", "gemini-2.0-flash"];
    case "ENTERPRISE":
      return ["claude-3.5-sonnet", "gpt-4o", "gemini-2.0-flash", "deepseek-chat"];
  }
};

// FINAL ROUTING
const routeToModel = (
  context: RequestContext,
  systemMetrics: any
): RoutingDecision => {
  const allowedModels = getTierConstraints(context.user.tier);

  const scores = allowedModels.map(model => ({
    model,
    quality: getQualityScore(model, context),
    cost: getCostScore(model, context.estimatedTokens),
    availability: getAvailabilityScore(model, systemMetrics),
  }));

  // Calculate final scores
  scores.forEach(score => {
    score.final =
      0.4 * score.quality +
      0.3 * score.cost +
      0.3 * score.availability;
  });

  // Sort by final score
  const sorted = scores.sort((a, b) => b.final - a.final);

  return {
    selectedModel: sorted[0].model,
    scores: sorted[0],
    reasoning: generateReasoning(sorted[0], context),
    fallbacks: sorted.slice(1).map(s => s.model),
  };
};
```

### Fallback Strategy

```typescript
const callModelWithFallback = async (
  routing: RoutingDecision,
  prompt: string,
  context: any
): Promise<Response> => {
  const models = [routing.selectedModel, ...routing.fallbacks];
  let lastError: Error | null = null;

  for (let i = 0; i < Math.min(models.length, 3); i++) {
    try {
      const model = models[i];

      // Set timeout: 30 seconds
      const response = await Promise.race([
        callLLM(model, prompt, context),
        new Promise((_, reject) =>
          setTimeout(() => reject(new Error("Timeout")), 30000)
        ),
      ]);

      return response;
    } catch (error) {
      lastError = error;
      console.log(`Model ${models[i]} failed, trying fallback...`);

      // Log attempt
      await logRoutingAttempt({
        requestId: context.requestId,
        model: models[i],
        success: false,
        error: error.message,
        attempt: i + 1,
      });
    }
  }

  // All models failed
  throw new Error(
    `All models failed after 3 attempts. Last error: ${lastError.message}`
  );
};
```

---

## ðŸ“ ESTRUCTURA DE CÃ“DIGO

```
src/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ auth/
â”‚   â”‚   â”‚   â”œâ”€â”€ [...nextauth]/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ route.ts              (NextAuth routes)
â”‚   â”‚   â”‚   â”œâ”€â”€ signin/page.tsx           (Login page)
â”‚   â”‚   â”‚   â”œâ”€â”€ signup/page.tsx           (Register page)
â”‚   â”‚   â”‚   â””â”€â”€ callback/[provider]/route.ts (OAuth callback)
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ v1/
â”‚   â”‚       â”œâ”€â”€ chat/
â”‚   â”‚       â”‚   â”œâ”€â”€ send/
â”‚   â”‚       â”‚   â”‚   â””â”€â”€ route.ts          (SSE streaming)
â”‚   â”‚       â”‚   â”œâ”€â”€ sessions/
â”‚   â”‚       â”‚   â”‚   â”œâ”€â”€ route.ts          (GET list, POST create)
â”‚   â”‚       â”‚   â”‚   â””â”€â”€ [id]/
â”‚   â”‚       â”‚   â”‚       â”œâ”€â”€ route.ts      (GET detail, DELETE)
â”‚   â”‚       â”‚   â”‚       â””â”€â”€ messages/route.ts
â”‚   â”‚       â”‚   â””â”€â”€ history/
â”‚   â”‚       â”‚       â””â”€â”€ [sessionId]/route.ts (GET paginated)
â”‚   â”‚       â”‚
â”‚   â”‚       â”œâ”€â”€ rag/
â”‚   â”‚       â”‚   â”œâ”€â”€ documents/
â”‚   â”‚       â”‚   â”‚   â”œâ”€â”€ route.ts          (POST upload, GET list)
â”‚   â”‚       â”‚   â”‚   â””â”€â”€ [id]/
â”‚   â”‚       â”‚   â”‚       â””â”€â”€ route.ts      (DELETE)
â”‚   â”‚       â”‚   â”œâ”€â”€ search/
â”‚   â”‚       â”‚   â”‚   â””â”€â”€ route.ts          (POST semantic search)
â”‚   â”‚       â”‚   â””â”€â”€ embeddings/
â”‚   â”‚       â”‚       â””â”€â”€ route.ts          (Internal: generate embeddings)
â”‚   â”‚       â”‚
â”‚   â”‚       â”œâ”€â”€ agents/
â”‚   â”‚       â”‚   â”œâ”€â”€ route.ts              (GET list)
â”‚   â”‚       â”‚   â”œâ”€â”€ [id]/route.ts         (GET detail)
â”‚   â”‚       â”‚   â”œâ”€â”€ [id]/enable/route.ts  (POST enable)
â”‚   â”‚       â”‚   â””â”€â”€ [id]/usage/route.ts   (GET usage stats)
â”‚   â”‚       â”‚
â”‚   â”‚       â””â”€â”€ health/
â”‚   â”‚           â””â”€â”€ route.ts              (GET health check)
â”‚   â”‚
â”‚   â”œâ”€â”€ middleware.ts                     (Auth + rate limiting)
â”‚   â”œâ”€â”€ layout.tsx
â”‚   â””â”€â”€ globals.css
â”‚
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ ai/
â”‚   â”‚   â”œâ”€â”€ router.ts                     (Hybrid routing logic)
â”‚   â”‚   â”œâ”€â”€ models.ts                     (Model configs)
â”‚   â”‚   â”œâ”€â”€ prompts.ts                    (System prompts)
â”‚   â”‚   â”œâ”€â”€ vercel-ai.ts                  (Vercel AI SDK client)
â”‚   â”‚   â””â”€â”€ tokens.ts                     (Token counting)
â”‚   â”‚
â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â”œâ”€â”€ embedding.ts                  (Embedding generation)
â”‚   â”‚   â”œâ”€â”€ chunking.ts                   (Document chunking)
â”‚   â”‚   â”œâ”€â”€ search.ts                     (Semantic search)
â”‚   â”‚   â”œâ”€â”€ pinecone.ts                   (Pinecone client)
â”‚   â”‚   â””â”€â”€ storage.ts                    (Vercel Blob)
â”‚   â”‚
â”‚   â”œâ”€â”€ auth/
â”‚   â”‚   â”œâ”€â”€ nextauth.ts                   (NextAuth configuration)
â”‚   â”‚   â”œâ”€â”€ rbac.ts                       (RBAC middleware)
â”‚   â”‚   â”œâ”€â”€ session.ts                    (Session utilities)
â”‚   â”‚   â””â”€â”€ guards.ts                     (requireAuth, requireTier, etc)
â”‚   â”‚
â”‚   â”œâ”€â”€ db/
â”‚   â”‚   â”œâ”€â”€ prisma.ts                     (Prisma client singleton)
â”‚   â”‚   â”œâ”€â”€ queries.ts                    (Common queries)
â”‚   â”‚   â”œâ”€â”€ transactions.ts               (Transaction helpers)
â”‚   â”‚   â””â”€â”€ seed.ts                       (Database seeding)
â”‚   â”‚
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ rate-limit.ts                 (Token bucket)
â”‚   â”‚   â”œâ”€â”€ error-handler.ts              (Error formatting)
â”‚   â”‚   â”œâ”€â”€ validators.ts                 (Zod schemas)
â”‚   â”‚   â””â”€â”€ response.ts                   (Response formatting)
â”‚   â”‚
â”‚   â”œâ”€â”€ logging/
â”‚   â”‚   â”œâ”€â”€ logger.ts                     (Winston setup)
â”‚   â”‚   â”œâ”€â”€ audit.ts                      (Audit logging)
â”‚   â”‚   â””â”€â”€ metrics.ts                    (Metrics tracking)
â”‚   â”‚
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ constants.ts
â”‚       â””â”€â”€ helpers.ts
â”‚
â”œâ”€â”€ types/
â”‚   â”œâ”€â”€ api.ts                            (API request/response types)
â”‚   â”œâ”€â”€ models.ts                         (Domain models)
â”‚   â”œâ”€â”€ db.ts                             (Database types)
â”‚   â”œâ”€â”€ auth.ts                           (Auth types)
â”‚   â””â”€â”€ llm.ts                            (LLM types)
â”‚
â”œâ”€â”€ __tests__/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ chat.test.ts
â”‚   â”‚   â”œâ”€â”€ rag.test.ts
â”‚   â”‚   â”œâ”€â”€ auth.test.ts
â”‚   â”‚   â””â”€â”€ agents.test.ts
â”‚   â”‚
â”‚   â”œâ”€â”€ integration/
â”‚   â”‚   â”œâ”€â”€ chat-flow.test.ts
â”‚   â”‚   â”œâ”€â”€ rag-pipeline.test.ts
â”‚   â”‚   â””â”€â”€ auth-flow.test.ts
â”‚   â”‚
â”‚   â”œâ”€â”€ unit/
â”‚   â”‚   â”œâ”€â”€ router.test.ts
â”‚   â”‚   â”œâ”€â”€ chunking.test.ts
â”‚   â”‚   â””â”€â”€ rate-limit.test.ts
â”‚   â”‚
â”‚   â””â”€â”€ e2e/
â”‚       â””â”€â”€ chat-complete-flow.test.ts
â”‚
â”œâ”€â”€ prisma/
â”‚   â”œâ”€â”€ schema.prisma
â”‚   â”œâ”€â”€ migrations/
â”‚   â””â”€â”€ seed.ts
â”‚
â”œâ”€â”€ .env.local                            (Local development)
â”œâ”€â”€ .env.production                       (Production config)
â”œâ”€â”€ .env.example
â”œâ”€â”€ jest.config.ts
â”œâ”€â”€ tsconfig.json
â””â”€â”€ package.json
```

---

## ðŸ“¦ DEPENDENCIAS Y SETUP

### package.json Dependencies

```json
{
  "dependencies": {
    "next": "^15.0.0",
    "react": "^18.2.0",
    "react-dom": "^18.2.0",
    "typescript": "^5.3.0",

    "ai": "^3.4.0",
    "@openai/api": "^4.50.0",
    "@anthropic-ai/sdk": "^0.20.0",
    "@google/generative-ai": "^0.10.0",
    "openai": "^4.52.0",

    "@pinecone-database/pinecone": "^3.0.0",
    "@prisma/client": "^5.7.0",

    "next-auth": "^5.0.0",
    "bcryptjs": "^2.4.3",
    "jsonwebtoken": "^9.1.0",

    "zod": "^3.22.0",
    "@hookform/resolvers": "^3.3.0",

    "redis": "^4.6.0",
    "ioredis": "^5.3.0",

    "winston": "^3.11.0",
    "dotenv": "^16.3.1",
    "cors": "^2.8.5"
  },
  "devDependencies": {
    "@types/node": "^20.10.0",
    "@types/react": "^18.2.0",
    "@types/jest": "^29.5.0",

    "jest": "^29.7.0",
    "@testing-library/react": "^14.1.0",
    "@testing-library/jest-dom": "^6.1.0",
    "supertest": "^6.3.0",
    "@types/supertest": "^6.0.0",

    "typescript-eslint": "^6.13.0",
    "eslint": "^8.55.0",
    "prettier": "^3.1.0",

    "tsx": "^4.7.0"
  }
}
```

### Environment Variables

```env
# .env.local
NODE_ENV=development
NEXT_PUBLIC_API_URL=http://localhost:3000/api/v1

# NextAuth
NEXTAUTH_URL=http://localhost:3000
NEXTAUTH_SECRET=$(openssl rand -base64 32)

# Database
DATABASE_URL=postgresql://user:password@localhost:5432/cjhirashi_agents_dev

# LLM APIs
ANTHROPIC_API_KEY=sk-ant-...
OPENAI_API_KEY=sk-...
GOOGLE_API_KEY=...
DEEPSEEK_API_KEY=...

# RAG
PINECONE_API_KEY=...
PINECONE_ENVIRONMENT=us-west-2

# Storage
BLOB_READ_WRITE_TOKEN=...

# OAuth
GOOGLE_CLIENT_ID=...
GOOGLE_CLIENT_SECRET=...
GITHUB_CLIENT_ID=...
GITHUB_CLIENT_SECRET=...

# Redis (for rate limiting)
REDIS_URL=redis://localhost:6379

# Logging
LOG_LEVEL=debug
```

### Initial Setup Commands

```bash
# 1. Clone repo (already done)
# 2. Install dependencies
npm install

# 3. Setup environment
cp .env.example .env.local
# Edit .env.local with real values

# 4. Setup database
npx prisma migrate dev --name init

# 5. Run development server
npm run dev

# 6. Run tests
npm run test

# 7. Build for production
npm run build
```

---

## ðŸ§ª TESTING STRATEGY

### Testing Pyramid

```
        /\
       /  \  E2E Tests (10%)
      /    \  - Full chat flow
     /      \ - Auth + RAG
    /        \ Integration (30%)
   /          \ - Chat + DB
  /            \ - RAG pipeline
 /              \ Unit Tests (60%)
/________________\ - Router, chunking, etc
```

### Test Coverage Goals

```
Overall Coverage: > 80%

By Component:
- API Endpoints:        > 85%
- LLM Router:          > 90%
- RAG Pipeline:        > 85%
- Auth + RBAC:         > 90%
- Database Queries:    > 80%
- Utils:               > 75%
```

### Test Examples

#### Unit Test: LLM Router

```typescript
// __tests__/unit/router.test.ts
describe("LLM Router - Hybrid Algorithm", () => {
  describe("Score Calculation", () => {
    it("should calculate quality score based on model", () => {
      const score = getQualityScore("claude-3.5-sonnet", context);
      expect(score).toBeGreaterThan(0.9);
    });

    it("should boost Claude score for code tasks", () => {
      const codeContext = { prompt: "write Python code" };
      const score = getQualityScore("claude-3.5-sonnet", codeContext);
      expect(score).toBeGreaterThan(0.95);
    });

    it("should penalize slow models", () => {
      const slowModelContext = { avgLatency: 3000 };
      const score = getAvailabilityScore("slow-model", metrics);
      expect(score).toBeLessThan(0.8);
    });
  });

  describe("Tier Constraints", () => {
    it("FREE tier should only use economic models", () => {
      const constraints = getTierConstraints("FREE");
      expect(constraints).toContain("gemini-2.0-flash");
      expect(constraints).not.toContain("gpt-4o");
    });
  });

  describe("Fallback Strategy", () => {
    it("should retry with fallback model if primary fails", async () => {
      const response = await callModelWithFallback(routing, prompt, context);
      expect(response).toBeDefined();
    });

    it("should fail after 3 retries", async () => {
      await expect(
        callModelWithFallback(routingAllFails, prompt, context)
      ).rejects.toThrow("All models failed");
    });
  });
});
```

#### Integration Test: Chat Flow

```typescript
// __tests__/integration/chat-flow.test.ts
describe("Chat Flow - Integration", () => {
  it("should create session, send message, and get response", async () => {
    // 1. Create session
    const sessionRes = await request(app)
      .post("/api/v1/chat/sessions")
      .set("Authorization", `Bearer ${token}`)
      .send({ title: "Test Chat" });

    expect(sessionRes.status).toBe(201);
    const sessionId = sessionRes.body.data.id;

    // 2. Send message with streaming
    const messageRes = await request(app)
      .post("/api/v1/chat/send")
      .set("Authorization", `Bearer ${token}`)
      .send({
        sessionId,
        message: "Hello, explain RAG",
        ragEnabled: true,
      });

    expect(messageRes.status).toBe(200);

    // 3. Verify message saved
    const messages = await db.message.findMany({ where: { sessionId } });
    expect(messages).toHaveLength(2); // user + assistant
  });

  it("should integrate RAG search with chat response", async () => {
    // Upload document
    const docRes = await uploadDocument(token, pdfFile);
    const docId = docRes.body.data.id;

    // Wait for indexing
    await sleep(2000);

    // Send question
    const chatRes = await request(app)
      .post("/api/v1/chat/send")
      .set("Authorization", `Bearer ${token}`)
      .send({
        sessionId,
        message: "What's in the document?",
        ragEnabled: true,
      });

    expect(chatRes.status).toBe(200);
    // Verify RAG context was used
  });
});
```

#### E2E Test: Complete Chat

```typescript
// __tests__/e2e/chat-complete-flow.test.ts
describe("E2E: Complete Chat Flow", () => {
  it("should handle full user journey: register -> chat -> rag -> response", async () => {
    // 1. Register
    const signupRes = await request(app)
      .post("/api/v1/auth/signup")
      .send({
        email: "test@example.com",
        password: "TestPassword123!",
        name: "Test User",
      });
    expect(signupRes.status).toBe(201);
    const token = signupRes.body.data.token;

    // 2. Create session
    const sessionRes = await request(app)
      .post("/api/v1/chat/sessions")
      .set("Authorization", `Bearer ${token}`);
    const sessionId = sessionRes.body.data.id;

    // 3. Upload RAG document
    const docRes = await request(app)
      .post("/api/v1/rag/documents")
      .set("Authorization", `Bearer ${token}`)
      .attach("file", pdfPath);
    expect(docRes.status).toBe(201);

    // 4. Send chat message
    const chatRes = await request(app)
      .post("/api/v1/chat/send")
      .set("Authorization", `Bearer ${token}`)
      .send({
        sessionId,
        message: "Summarize the document",
        ragEnabled: true,
      });
    expect(chatRes.status).toBe(200);

    // 5. Get response
    const response = chatRes.body.data;
    expect(response.content).toBeTruthy();
    expect(response.tokensUsed).toBeGreaterThan(0);
    expect(response.cost).toBeGreaterThan(0);
  });
});
```

---

## âœ… CRITERIOS DE FINALIZACIÃ“N

### Criterios TÃ©cnicos

- [ ] **Chat API Funcional**
  - [ ] POST /api/v1/chat/send retorna SSE streaming
  - [ ] Streaming incluye eventos: start, chunk, done
  - [ ] Response headers correctos
  - [ ] Manejo de errores apropiado

- [ ] **All 58 Endpoints Implemented**
  - [ ] Authentication: 6/6 âœ“
  - [ ] Chat: 6/6 âœ“
  - [ ] Agents: 5/5 âœ“
  - [ ] RAG: 4/4 âœ“
  - [ ] Artifacts: 4/4 âœ“
  - [ ] Users: 5/5 âœ“
  - [ ] Admin: 4/4 âœ“
  - [ ] Support: 3/3 âœ“
  - [ ] MCP: 4/4 âœ“
  - [ ] Storage: 5/5 âœ“
  - [ ] Health: 6/6 âœ“
  - [ ] Finance: 6/6 âœ“

- [ ] **LLM Routing Hybrid Working**
  - [ ] Scores calculados correctamente
  - [ ] Tier constraints aplicados
  - [ ] Fallback strategy funciona
  - [ ] Timeout: 30s implementado
  - [ ] Cost tracking funciona

- [ ] **RAG Pipeline Funcional**
  - [ ] Documento upload working
  - [ ] Chunking preserva metadata
  - [ ] Embedding generation OK
  - [ ] Pinecone storage OK
  - [ ] Semantic search OK (threshold 0.7)
  - [ ] Context injection en prompts

- [ ] **Authentication & Security**
  - [ ] NextAuth setup completo
  - [ ] JWT tokens generados
  - [ ] RBAC middleware funciona
  - [ ] Rate limiting enforced
  - [ ] No secrets en logs

- [ ] **Testing Coverage > 80%**
  - [ ] Unit tests: 60%+
  - [ ] Integration tests: 30%+
  - [ ] E2E tests: 10%+
  - [ ] All critical paths tested
  - [ ] Error scenarios covered

### Criterios de Quality

- [ ] TypeScript strict mode: 0 errors
- [ ] ESLint: 0 warnings
- [ ] Prettier: code formatted
- [ ] No console.logs en production code
- [ ] Logging strategy implemented
- [ ] Error messages user-friendly
- [ ] Performance: <2s latency p95
- [ ] Database queries optimized
- [ ] Zero N+1 queries

### Criterios de Documentation

- [ ] API documentation updated
- [ ] Code comments for complex logic
- [ ] README for backend setup
- [ ] Environment variables documented
- [ ] Database migration guide
- [ ] Deployment checklist

### Sign-Off Criteria

- [ ] All tests passing
- [ ] Code reviewed + approved
- [ ] Security review passed
- [ ] Performance SLAs met
- [ ] Documentation complete
- [ ] Ready for Fase 6 (Frontend)

---

## âš ï¸ RIESGOS Y MITIGACIÃ“N

| Riesgo | Probabilidad | Impacto | MitigaciÃ³n |
|--------|-------------|--------|-----------|
| LLM API rate limits/outages | Media | Alto | Fallback strategy, queue system, monitoring |
| Pinecone vector search slow | Baja | Medio | Proper indexing, caching, pre-computed embeddings |
| Large documents chunking issues | Baja | Medio | Robust parsing, fallback to simple chunking |
| SSE streaming connection drops | Media | Bajo | Automatic reconnect logic, client-side retry |
| Database connection pool exhaustion | Baja | Alto | Connection pooling, monitoring, alerts |
| Token counting inaccuracies | Baja | Medio | Validate against actual API response |
| Memory leak in streaming | Baja | Medio | Proper cleanup, memory monitoring |
| Auth token expiration edge cases | Media | Bajo | Refresh token logic, session refresh |

---

## ðŸ“… CRONOGRAMA DETALLADO

### Semana 1: Infrastructure + Chat Basics

**Coder:**
- Day 1-2: Setup infraestructura (Prisma, Vercel AI, env)
- Day 3-5: Implementar Chat API bÃ¡sico (send, sessions CRUD)

**AI-Specialist:**
- Day 1-2: Hybrid router skeleton
- Day 3-5: Empezar integraciones modelo (Claude)

**Architect:**
- Daily: Oversight, code review, blocker resolution

**Deliverables:**
- Setup funcionando
- Chat API bÃ¡sica con 1 modelo
- Tests setup

### Semana 2: LLM Routing + RAG Pipeline

**Coder:**
- Day 1-3: NextAuth setup + RBAC
- Day 4-5: Rate limiting

**AI-Specialist:**
- Day 1-5: Complete router + RAG (embedding, chunking, search)

**Deliverables:**
- Hybrid router fully working
- 4 modelos integrados
- RAG pipeline completo
- Auth + rate limiting working

### Semana 3: Testing + Polish

**Coder:**
- Day 1-4: Tests (unit, integration, E2E)
- Day 5: Bug fixes, optimization

**Tester:**
- Day 1-5: Test planning, validation, edge cases

**Architect:**
- Daily: Final review, performance optimization

**Deliverables:**
- >80% test coverage
- All SLAs met
- Documentation complete
- Production ready

---

## ðŸš€ LISTO PARA DELEGAR

Este plan estÃ¡ listo para presentar a:
1. **coder** - Backend implementation lead
2. **ai-specialist** - LLM routing + RAG
3. **architect** - Oversight + validation
4. **tester** - Test strategy
5. **code-reviewer** - Quality assurance

**PrÃ³ximo paso:** Crear tasks especÃ­ficas para cada especialista basadas en este plan.

---

**FASE 5 IMPLEMENTATION PLAN READY FOR EXECUTION**

Documento generado: 2025-10-22
Estado: âœ… APROBADO PARA DELEGACIÃ“N
